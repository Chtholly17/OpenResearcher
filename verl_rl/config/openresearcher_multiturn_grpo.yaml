hydra:
  searchpath:
    - file://verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

# ---- Data ----
data:
  # Path to parquet files generated by preprocess_openresearcher.py
  train_files: ~/data/openresearcher/train.parquet
  val_files: ~/data/openresearcher/test.parquet
  max_prompt_length: 2048
  # Deep research trajectories are long; adjust based on GPU memory
  max_response_length: 8192
  train_batch_size: 64
  # Required for multi-turn tool-calling workflows
  return_raw_chat: True

# ---- Reward ----
reward:
  # Use our custom reward function instead of verl's default_compute_score,
  # because our data_source "OpenResearcher/OpenResearcher" is not in verl's
  # built-in dispatch table.
  custom_reward_function:
    path: verl_rl/reward/openresearcher_reward.py
    name: compute_score

# ---- Actor / Rollout / Reference ----
actor_rollout_ref:
  hybrid_engine: True
  model:
    path: OpenResearcher/OpenResearcher-30B-A3B
    enable_gradient_checkpointing: True
  actor:
    # LoRA for memory efficiency on 30B model
    use_lora: True
    lora_rank: 64
    lora_alpha: 64
    optim:
      lr: 1e-6
      weight_decay: 0.01
    ppo_mini_batch_size: 16
    ppo_micro_batch_size_per_gpu: 2
    ppo_epochs: 1
    clip_ratio: 0.2
    entropy_coeff: 0.01
  ref:
    log_prob_micro_batch_size_per_gpu: 2
  rollout:
    name: sglang
    temperature: 1.0
    top_p: 0.95
    stop_token_ids: []
    # Multi-turn configuration
    multi_turn:
      enable: True
      # Max number of assistant generation turns (search->open->find->... cycles)
      max_assistant_turns: 30
      # Max user/interaction turns (for interaction-based feedback)
      max_user_turns: 5
      # Max parallel tool calls per turn
      max_parallel_calls: 1
      # Tool response length limit (search results are long, need generous limit)
      max_tool_response_length: 2048
      # Truncate from the middle to keep both start and end of results
      tool_response_truncate_side: middle
      # Tool call format: hermes uses <tool_call>...</tool_call> tags,
      # which matches the OpenResearcher model's native format
      format: hermes
      # Tool config pointing to our browser tools
      tool_config_path: ./config/tool_config/openresearcher_tool_config.yaml
      # Interaction config for trajectory-level feedback
      interaction_config_path: ./config/interaction_config/openresearcher_interaction_config.yaml
    # Server mode for SGLang inference server
    sglang_rollout_mode: server
    server:
      timeout: 120
      max_attempts: 3
      retry_delay: 5
      max_connections: 500
      max_start_wait_time: 600.0

# ---- Algorithm ----
algorithm:
  adv_estimator: grpo
  # GRPO-specific: number of completions to generate per prompt
  # Higher = better reward signal estimation, but more compute
  grpo_n_generations: 4
  norm_adv_by_std_in_grpo: True
  gamma: 1.0
  lam: 1.0
  # KL penalty (keep low for exploration)
  use_kl_in_reward: False
  kl_ctrl:
    type: fixed
    kl_coef: 0.001

# ---- Trainer ----
trainer:
  total_epochs: 5
  total_training_steps: null
  project_name: openresearcher_rl
  experiment_name: grpo_multiturn
  logger: ["console", "wandb"]
  nnodes: 1
  n_gpus_per_node: 8
  save_freq: 50
  test_freq: 25
  val_before_train: True
  default_local_dir: checkpoints/${trainer.project_name}/${trainer.experiment_name}
