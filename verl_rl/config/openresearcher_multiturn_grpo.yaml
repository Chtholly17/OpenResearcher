hydra:
  searchpath:
    - pkg://verl.trainer.config

defaults:
  - ppo_trainer
  - _self_

# ---- Data ----
data:
  # Path to parquet files generated by preprocess_openresearcher.py
  train_files: ~/data/openresearcher/train.parquet
  val_files: ~/data/openresearcher/test.parquet
  max_prompt_length: 2048
  # Deep research trajectories are long; adjust based on GPU memory
  max_response_length: 8192
  train_batch_size: 64
  # Required for multi-turn tool-calling workflows
  return_raw_chat: True

# ---- Reward ----
# custom_reward_function is a top-level key in verl's ppo_trainer config,
# NOT nested under "reward". Our data_source "OpenResearcher/OpenResearcher"
# is not in verl's built-in dispatch table, so we must provide our own.
custom_reward_function:
  path: verl_rl/reward/openresearcher_reward.py
  name: compute_score

# ---- Actor / Rollout / Reference ----
actor_rollout_ref:
  hybrid_engine: True
  model:
    path: OpenResearcher/OpenResearcher-30B-A3B
    trust_remote_code: True
    enable_gradient_checkpointing: True
  actor:
    optim:
      lr: 1e-6
      weight_decay: 0.01
    ppo_mini_batch_size: 16
    ppo_micro_batch_size_per_gpu: 2
    ppo_epochs: 1
    clip_ratio: 0.2
    entropy_coeff: 0.01
  ref:
    log_prob_micro_batch_size_per_gpu: 2
  rollout:
    name: vllm
    temperature: 1.0
    top_p: 0.95
    # Multi-turn configuration
    multi_turn:
      enable: True
      # Max number of assistant generation turns (search->open->find->... cycles)
      # Keep moderate to leave token budget for answer submission.
      max_assistant_turns: 15
      # Max user/interaction turns (for interaction-based feedback)
      max_user_turns: 5
      # Max parallel tool calls per turn
      max_parallel_calls: 1
      # Tool response length limit (search results are long, need generous limit)
      max_tool_response_length: 2048
      # Truncate from the middle to keep both start and end of results
      tool_response_truncate_side: middle
      # Tool call format: nemotron uses <tool_call><function=...><parameter=...>
      # XML tags, which matches the Nemotron chat template's native format.
      # The built-in "hermes" parser expects JSON inside <tool_call> tags and
      # silently fails on this model. Our custom parser is registered via
      # verl_rl.parsers.nemotron_tool_parser.
      format: nemotron
      # Tool config pointing to our browser tools
      tool_config_path: ./config/tool_config/openresearcher_tool_config.yaml
      # Interaction config for trajectory-level feedback
      interaction_config_path: ./config/interaction_config/openresearcher_interaction_config.yaml
    server:
      timeout: 120
      max_attempts: 3
      retry_delay: 5
      max_connections: 500
      max_start_wait_time: 600.0

# ---- Algorithm ----
algorithm:
  adv_estimator: grpo
  # GRPO-specific: number of completions to generate per prompt
  # Higher = better reward signal estimation, but more compute
  grpo_n_generations: 4
  norm_adv_by_std_in_grpo: True
  gamma: 1.0
  lam: 1.0
  # KL penalty (keep low for exploration)
  use_kl_in_reward: False
  kl_ctrl:
    type: fixed
    kl_coef: 0.001

# ---- Trainer ----
trainer:
  total_epochs: 5
  total_training_steps: null
  project_name: openresearcher_rl
  experiment_name: grpo_multiturn
  logger: ["console", "wandb"]
  nnodes: 1
  n_gpus_per_node: 8
  save_freq: 50
  test_freq: 25
  val_before_train: True
  default_local_dir: checkpoints/${trainer.project_name}/${trainer.experiment_name}
